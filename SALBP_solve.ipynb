{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T17:49:55.341329Z",
     "start_time": "2025-06-10T17:49:54.260241Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_alb(alb_file_name):\n",
    "    \"\"\"Reads assembly line balancing instance .alb file, returns dictionary with the information\"\"\"\n",
    "    parse_dict = {}\n",
    "    alb_file = open(alb_file_name).read()\n",
    "    # Get number of tasks\n",
    "    num_tasks = re.search(\"<number of tasks>\\n(\\\\d*)\", alb_file)\n",
    "    parse_dict[\"num_tasks\"] = int(num_tasks.group(1))\n",
    "\n",
    "    # Get cycle time\n",
    "    cycle_time = re.search(\"<cycle time>\\n(\\\\d*)\", alb_file)\n",
    "    parse_dict[\"cycle_time\"] = int(cycle_time.group(1))\n",
    "\n",
    "    # Order Strength\n",
    "    order_strength = re.search(\"<order strength>\\n(\\\\d*,\\\\d*)\", alb_file)\n",
    "    \n",
    "    if order_strength:\n",
    "        parse_dict[\"original_order_strength\"] = float(order_strength.group(1).replace(\",\", \".\"))\n",
    "    else:\n",
    "        order_strength = re.search(\"<order strength>\\n(\\\\d*.\\\\d*)\", alb_file)\n",
    "        parse_dict[\"original_order_strength\"] = float(order_strength.group(1))\n",
    "\n",
    "    # Task_times\n",
    "    task_times = re.search(\"<task times>(.|\\n)+?<\", alb_file)\n",
    "\n",
    "    # Get lines in this regex ignoring the first and last 2\n",
    "    task_times = task_times.group(0).split(\"\\n\")[1:-2]\n",
    "    task_times = {task.split()[0]: int(task.split()[1]) for task in task_times}\n",
    "    parse_dict[\"task_times\"] = task_times\n",
    "\n",
    "    # Precedence relations\n",
    "    precedence_relations = re.search(\"<precedence relations>(.|\\n)+?<\", alb_file)\n",
    "    precedence_relations = precedence_relations.group(0).split(\"\\n\")[1:-2]\n",
    "    precedence_relations = [task.split(\",\") for task in precedence_relations]\n",
    "    parse_dict[\"precedence_relations\"] = precedence_relations\n",
    "    return parse_dict\n",
    "\n",
    "def write_to_alb(salbp_dict, alb_file_name):\n",
    "    \"\"\"Writes the SALBP dictionary to an .alb file\"\"\"\n",
    "    #Format of alb:\n",
    "    # <number of tasks>\n",
    "    # no_tasks\n",
    "    # <cycle time>\n",
    "    # cycle_time\n",
    "    #<task times>\n",
    "    #task_id task_time\n",
    "    #<precedence relations>\n",
    "    #task_id,task_id\n",
    "\n",
    "\n",
    "    # Write number of tasks\n",
    "    alb = \"<number of tasks>\\n\"\n",
    "    alb += str(salbp_dict[\"num_tasks\"]) + \"\\n\"\n",
    "    # Write cycle time\n",
    "    alb += \"<cycle time>\\n\"\n",
    "    alb += str(salbp_dict[\"cycle_time\"]) + \"\\n\"\n",
    "    # Write task times\n",
    "    alb += \"<task times>\\n\"\n",
    "    for task_id, task_time in salbp_dict[\"task_times\"].items():\n",
    "        alb += task_id + \" \" + str(task_time) + \"\\n\"\n",
    "    # Write precedence relations\n",
    "    alb += \"<precedence relations>\\n\"\n",
    "    for relation in salbp_dict[\"precedence_relations\"]:\n",
    "        alb += relation[0] + \",\" + relation[1] + \"\\n\"\n",
    "    #ends the file\n",
    "    alb += \"<end>\"\n",
    "    with open(alb_file_name, \"w\") as alb_file:\n",
    "        alb_file.write(alb)\n",
    "    \n",
    "\n",
    "\n",
    "SALBP_dict = parse_alb(\"/Users/letshopethisworks2/Documents/phd_paper_material/MALBP_instance_generation/SALBP_benchmark/small data set_n=20/instance_n=20_1.alb\")\n",
    "write_to_alb(SALBP_dict, \"test.alb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_alb = {'num_tasks': 12,\n",
    " 'cycle_time': 10,\n",
    " 'original_order_strength': 0.268,\n",
    " 'task_times': {'1': 3,\n",
    "  '2': 8,\n",
    "  '3': 3,\n",
    "  '4': 2,\n",
    "  '5': 4,\n",
    "  '6': 2,\n",
    "  '7': 5,\n",
    "  '8': 1,\n",
    "  '9': 9,\n",
    "  '10': 4,\n",
    "  '11': 6,\n",
    "  '12': 2,\n",
    "    },\n",
    " 'precedence_relations': [\n",
    "     ['1', '5'],\n",
    "  ['2', '5'],\n",
    "  ['3', '5'],\n",
    "  ['3', '4'],\n",
    "  ['4', '9'],\n",
    "  ['5', '6'],\n",
    "  ['5', '7'],\n",
    "  ['5', '8'],\n",
    "  ['6', '11'],\n",
    "  ['7', '11'],\n",
    "  ['8', '11'],\n",
    "  ['9', '10'],\n",
    "  ['10', '12'],\n",
    "  ['11', '12']\n",
    "  ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SALBP_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls BBR-for-SALBP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: ./salb_d probfile\n",
      "    -b: 1 = use bin packing LB, 0 = do not use (def=0)\n",
      "    -m: method (search_strategy) to use during Phase II 1 = DBFS, 2 = BFS (def=1)\n",
      "    -p: controls level of printed information (def=0)\n",
      "    -s: seed for random number generation\n",
      "    -t: CPU time limit\n",
      "    -u: Optional initial upper bound\n",
      "    -d: Set direction (-1 = auto (default), 0 = reverse, 1 = forward)\n"
     ]
    }
   ],
   "source": [
    "! ../BBR-for-SALBP1/SALB/SALB/salb  -p 2 \"test.alb\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.alb\n",
      "n=50\n",
      "cycle time=1000\n",
      "t1=384 t2=493 t3=696 t4=566 t5=576 t6=696 t7=637 t8=804 t9=604 t10=492 t11=551 t12=788 t13=320 t14=583 t15=708 t16=558 t17=564 t18=592 t19=600 t20=586 t21=611 t22=483 t23=417 t24=463 t25=343 t26=461 t27=578 t28=427 t29=518 t30=575 t31=628 t32=442 t33=405 t34=430 t35=334 t36=452 t37=371 t38=574 t39=442 t40=366 t41=196 t42=390 t43=529 t44=372 t45=698 t46=564 t47=489 t48=330 t49=373 t50=581 \n",
      "running forward 2383920 56711200\n",
      "MHH upper bound: 29 (0.00s)\n",
      "First lower bound: 26\n",
      "Second lower bound 27\n",
      "lower bound time: 0.00s\n",
      "bin packing time: 0.00s\n",
      "     10000      10908      20908 16 1489  28     92.623\n",
      "     20000      20221      40221 18 1642  32     90.862\n",
      "     30000      30902      60902 11 2189  16    198.320\n",
      "     40000      38437      78437 16 1619  28    100.748\n",
      "     50000      44759      94759 16 1655  28    102.998\n",
      "     60000      53027     113027 20 1813  36     90.370\n",
      "     70000      61607     131607 13 1769  21    135.497\n",
      "     80000      68162     148162 18 1804  32     99.862\n",
      "     90000      72468     162468 19 1816  34     95.259\n",
      "    100000      77268     177268 18 1822  31    100.842\n",
      "    110000      81153     191153 15 1736  26    115.253\n",
      "    120000      83995     203995 17 1778  30    104.188\n",
      "    130000      86783     216783 13 1946  21    149.112\n",
      "    140000      90205     230205 19 1922  34    100.838\n",
      "    150000      96719     246719 18 1834  31    101.509\n",
      "    160000     101996     261996 15 1776  25    117.900\n",
      "    170000     105838     275838 17 1809  30    106.012\n",
      "    180000     108701     288701 18 1876  31    103.842\n",
      "    190000     109960     299960 15 1796  25    119.233\n",
      "    200000     110361     310361 13 2239  20    171.631\n",
      "    210000     109873     319873 14 1876  23    133.460\n",
      "    220000     108618     328618 18 1899  31    105.120\n",
      "    230000     108354     338354 19 2023  33    106.134\n",
      "    240000     107450     347450 17 1846  30    108.188\n",
      "    250000     106328     356328 14 1951  23    138.817\n",
      "    260000     105122     365122 17 1857  29    108.815\n",
      "    270000     104418     374418 18 1935  31    107.120\n",
      "    280000     105715     385715 15 1863  25    123.700\n",
      "    290000     108549     398549 15 1872  25    124.300\n",
      "    300000     110450     410450 18 1947  31    107.787\n",
      "    310000     111865     421865 16 1861  28    115.873\n",
      "    320000     112573     432573 15 1898  26    126.053\n",
      "    330000     112488     442488 17 1895  30    111.071\n",
      "    340000     111853     451853 17 1885  29    110.462\n",
      "    350000     110312     460312 16 1885  27    117.353\n",
      "    360000     108454     468454 14 2219  22    157.940\n",
      "    370000     105659     475659 16 1898  27    118.165\n",
      "    380000     102343     482343 19 2076  33    108.923\n",
      "    390000      98712     488712 14 2274  22    161.869\n",
      "    400000      94721     494721 14 2297  22    163.511\n",
      "    410000      90381     500381 17 1942  29    113.815\n",
      "    420000      87173     507173 15 2050  25    136.167\n",
      "    430000      86481     516481 17 1959  30    114.835\n",
      "    440000      86794     526794 16 1956  27    121.790\n",
      "    450000      86962     536962 16 1968  27    122.540\n",
      "    460000      86984     546984 17 1989  29    116.580\n",
      "    470000      86740     556740 16 1995  27    124.228\n",
      "    480000      85749     565749 17 2013  29    117.992\n",
      "    490000      84295     574295 19 2092  33    109.765\n",
      "    500000      82295     582295 17 2041  29    119.639\n",
      "    510000      80426     590426 17 2057  29    120.580\n",
      "    520000      78916     598916 21 2264  36    107.530\n",
      "    530000      77766     607766 15 2239  24    148.747\n",
      "    540000      77034     617034 18 2153  31    119.231\n",
      "    550000      76483     626483 17 2121  29    124.345\n",
      "    560000      76143     636143 19 2251  33    118.134\n",
      "    570000      75268     645268 16 2155  27    134.227\n",
      "    580000      74081     654081 17 2156  29    126.404\n",
      "    590000      72959     662959 17 2166  29    126.992\n",
      "    600000      71222     671222 17 2176  29    127.580\n",
      "    610000      68913     678913 16 2190  26    136.395\n",
      "    620000      66350     686350 19 2345  32    123.061\n",
      "    630000      65185     695185 17 2205  28    129.266\n",
      "    640000      63141     703141 17 2215  29    129.874\n",
      "    650000      60496     710496 19 2290  33    120.186\n",
      "    660000      56869     716869 19 2344  32    123.008\n",
      "    670000      52366     722366 18 2291  30    126.878\n",
      "    680000      46987     726987 18 2327  30    128.878\n",
      "    690000      40489     730489 17 2266  28    132.854\n",
      "    700000      33450     733450 18 2360  30    130.711\n",
      "    710000      26148     736148 16 2304  26    143.520\n",
      "    720000      17794     737794 17 2305  28    135.148\n",
      "    730000       8446     738446 16 2355  26    146.708\n",
      "Solution with 30 stations\n",
      " 1\t1\n",
      "2\t2\n",
      "3\t7\n",
      "4\t19\n",
      "5\t1\n",
      "6\t25\n",
      "7\t22\n",
      "8\t4\n",
      "9\t20\n",
      "10\t2\n",
      "11\t3\n",
      "12\t5\n",
      "13\t3\n",
      "14\t16\n",
      "15\t6\n",
      "16\t9\n",
      "17\t27\n",
      "18\t17\n",
      "19\t28\n",
      "20\t15\n",
      "21\t8\n",
      "22\t30\n",
      "23\t16\n",
      "24\t12\n",
      "25\t21\n",
      "26\t29\n",
      "27\t10\n",
      "28\t9\n",
      "29\t18\n",
      "30\t26\n",
      "31\t13\n",
      "32\t18\n",
      "33\t17\n",
      "34\t19\n",
      "35\t13\n",
      "36\t12\n",
      "37\t10\n",
      "38\t11\n",
      "39\t21\n",
      "40\t11\n",
      "41\t21\n",
      "42\t20\n",
      "43\t29\n",
      "44\t23\n",
      "45\t14\n",
      "46\t24\n",
      "47\t30\n",
      "48\t22\n",
      "49\t24\n",
      "50\t23\n",
      "test test test.sol\n",
      "   verified_optimality = 1; value = 29; cpu = 16.22\n",
      "Hoffman cpu =   0.00  best_first_bbr cpu =  16.87  bfs_bbr cpu =   0.00 find_insert_cpu =   5.04  bin_cpu =   0.00  cpu =  16.22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SALBP_dict = parse_alb(\"/Users/letshopethisworks2/Documents/phd_paper_material/MMABPWW/SALBP_benchmark/medium data set_n=50/instance_n=50_331.alb\")\n",
    "write_to_alb(SALBP_dict, \"test.alb\")\n",
    "\n",
    "! ./salb_d \"test.alb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def random_task_time_change(SALBP_dict, multiplier = 1.5):\n",
    "    \"\"\"Increases a random task time by 1\"\"\"\n",
    "    import random\n",
    "    task_id = random.choice(list(SALBP_dict[\"task_times\"].keys()))\n",
    "    SALBP_dict[\"task_times\"][task_id] *= multiplier\n",
    "    return SALBP_dict\n",
    "\n",
    "def task_time_change(SALBP_dict, task_id, multiplier = 1.5, debug = False):\n",
    "    \"\"\"Increases a random task time by 1\"\"\"\n",
    "    if debug:\n",
    "        print(\"Changing task\", task_id, \"time by\", multiplier)\n",
    "    SALBP_dict[\"task_times\"][task_id] *= multiplier\n",
    "    return SALBP_dict\n",
    "\n",
    "def precedence_removal(SALBP_dict, edge_index):\n",
    "    \"\"\"Removes a precedence relation\"\"\"\n",
    "    SALBP_dict[\"precedence_relations\"].pop(edge_index)\n",
    "    return SALBP_dict\n",
    "    \n",
    "\n",
    "def parse_bb_salb1_out(text):\n",
    "    '''gets the number of stations, optimal flag and cpu time from the output of the salb1 program'''\n",
    "    output = text.stdout.decode(\"utf-8\")\n",
    "    # Regular expression to capture the required values\n",
    "    match = re.search(r\"verified_optimality\\s*=\\s*(\\d+);\\s*value\\s*=\\s*(\\d+);\\s*cpu\\s*=\\s*([\\d.]+)\", output)\n",
    "\n",
    "    if match:\n",
    "        verified_optimality = int(match.group(1))\n",
    "        value = int(match.group(2))\n",
    "        cpu = float(match.group(3))\n",
    "\n",
    "    else:\n",
    "        print(\"Pattern not found.\")\n",
    "    return value, verified_optimality, cpu\n",
    "\n",
    "def generate_results(fp = \"/Users/letshopethisworks2/Documents/phd_paper_material/MALBP_instance_generation/SALBP_benchmark/small data set_n=20/\" ,  instance_name = \"instance_n=20_\", ext = \".alb\", start=1, stop = 300):\n",
    "    results = []\n",
    "    for i in range(start,stop):\n",
    "        SALBP_dict_orig = parse_alb(f\"{fp}{instance_name}{i}{ext}\")\n",
    "        bin_dict = deepcopy(SALBP_dict_orig)\n",
    "        print(\"Running instance: \", i)\n",
    "        for j in range(len(SALBP_dict_orig[\"precedence_relations\"])):\n",
    "            SALBP_dict = deepcopy(SALBP_dict_orig)\n",
    "            SALBP_dict =precedence_removal(SALBP_dict, j)\n",
    "            write_to_alb(SALBP_dict, \"test.alb\")\n",
    "            output = subprocess.run([ex_fp, \"test.alb\"], stdout=subprocess.PIPE)\n",
    "            no_stations, optimal, cpu = parse_bb_salb1_out(output)\n",
    "            result = {\"instance:\": f\"instance_n=20_{i}\", \"precedence_relation\": j, \"no_stations\": no_stations, \"optimal\": optimal, \"cpu\": cpu}\n",
    "            save_backup(backup_name, result)\n",
    "            results.append(result)\n",
    "\n",
    "        #calculates bin packing lower bound\n",
    "        bin_dict['precedence_relations'] = []\n",
    "        write_to_alb(bin_dict, \"test.alb\")\n",
    "        output = subprocess.run([ex_fp, \"test.alb\"], stdout=subprocess.PIPE)\n",
    "        no_stations, optimal, cpu = parse_bb_salb1_out(output)\n",
    "        result = {\"instance:\": f\"instance_n=20_{i}\", \"precedence_relation\": \"None\", \"no_stations\": no_stations, \"optimal\": optimal, \"cpu\": cpu}\n",
    "        save_backup(backup_name, result)\n",
    "            \n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "#reads the results csv\n",
    "#results_df = pd.read_csv(\"task_20_bin_lb.csv\")\n",
    "#results_df = pd.DataFrame(results)\n",
    "#saves the results df to a csv file\n",
    "#results_df.to_csv(\"tasks20_test.csv\")\n",
    "# results = generate_results(start=400, stop = 525)\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(\"tasks20_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = pd.read_csv(\"task_20_bin_lb.csv\")\n",
    "res_2 = pd.read_csv(\"tasks20_2.csv\")\n",
    "res_3 = pd.read_csv(\"tasks20_3.csv\")\n",
    "\n",
    "results_df = pd.concat([res_1, res_2, res_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_df = results_df[results_df[\"precedence_relation\"].isna() == True].copy()\n",
    "#removes the rows with None precedence relations\n",
    "results_df = results_df[results_df['precedence_relation'].isna() == False]\n",
    "\n",
    "lb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the min and max number of stations for each instance\n",
    "min_and_max = results_df.groupby(\"instance:\")[\"no_stations\"].agg([\"min\", \"max\"])\n",
    "min_and_max.reset_index(inplace = True)\n",
    "#adds in lb values\n",
    "lb_df['bin_lb'] = lb_df['no_stations']\n",
    "min_and_max = pd.merge(min_and_max, lb_df[[\"instance:\", \"bin_lb\"]], on = \"instance:\")\n",
    "min_and_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts the number of times min does not equal max\n",
    "min_and_max[\"min_not_equal_max\"] = min_and_max[\"min\"] != min_and_max[\"max\"]\n",
    "min_and_max[\"min_not_equal_max\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts the number of time the bin_lb is less than the min\n",
    "min_and_max[\"bin_lb_less_than_min\"] = min_and_max[\"bin_lb\"] < min_and_max[\"min\"]\n",
    "min_and_max[\"bin_lb_less_than_min\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts the number of time the bin_lb is less than the max\n",
    "min_and_max[\"bin_lb_less_than_max\"] = min_and_max[\"bin_lb\"] < min_and_max[\"max\"]\n",
    "print(\"bin lb dif\", min_and_max[\"bin_lb_less_than_max\"].sum())\n",
    "#filters for the instances where the bin_lb is les than the max\n",
    "min_and_max[min_and_max[\"bin_lb_less_than_max\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the instances where min does not equal max\n",
    "interesting_instances = min_and_max[min_and_max[\"min_not_equal_max\"]]\n",
    "interesting_instances['instance:'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_20_101 = results_df[results_df[\"instance:\"] == \"instance_n=20_101\"]\n",
    "inst_20_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_salbp_graph(SALBP_dict):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(SALBP_dict[\"task_times\"].keys())\n",
    "    G.add_edges_from(SALBP_dict[\"precedence_relations\"])\n",
    "    #prints the edges\n",
    "    print(\"from dict\", SALBP_dict[\"precedence_relations\"])\n",
    "    #prints the edges from the graph\n",
    "    print(\"from graph\", G.edges())\n",
    "    nx.draw(G, with_labels = True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_salbp_edge_removal_graph(SALBP_dict, instance_name, res_df):\n",
    "    '''Colors the edges by the number of stations in res_df'''\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(SALBP_dict[\"task_times\"].keys())\n",
    "    G.add_edges_from(SALBP_dict[\"precedence_relations\"])\n",
    "    edge_colors = []\n",
    "    for edge in G.edges():\n",
    "        edge_index = SALBP_dict[\"precedence_relations\"].index(list(edge))\n",
    "        no_stations = res_df[(res_df[\"instance:\"] == instance_name) & (res_df[\"precedence_relation\"] == edge_index)][\"no_stations\"].values[0]\n",
    "        edge_colors.append(no_stations)\n",
    "    #saves edge colors as graph attribute\n",
    "    nx.set_edge_attributes(G, dict(zip(G.edges(), edge_colors)), \"value\")\n",
    "    pos = nx.nx_pydot.graphviz_layout(G, prog = \"dot\")\n",
    "   # Define colormap\n",
    "    unique_values = list(set(edge_colors))\n",
    "    print(unique_values)\n",
    "    color_map = cm.get_cmap('viridis', len(unique_values))\n",
    "    print(\"color map\", color_map)\n",
    "    cmap = mcolors.ListedColormap([color_map(val) for val in unique_values])\n",
    "\n",
    "    # Draw graph\n",
    "    #creates ax\n",
    "    fig, ax = plt.subplots()\n",
    "    edges = nx.draw_networkx_edges(G, pos, edge_color=edge_colors, edge_cmap=cmap, edge_vmin=min(edge_colors), edge_vmax=max(edge_colors), ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax)\n",
    "\n",
    "    # Add colorbar\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color = color_map(val), label=val, markersize=10) for val in unique_values]\n",
    "    plt.legend(handles=handles, loc=\"best\")\n",
    "\n",
    "    plt.show()\n",
    "    return G\n",
    "\n",
    "\n",
    "def draw_graph_with_discrete_legend(SALBP_dict, res_df, instance_name,  ax=None):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(SALBP_dict[\"task_times\"].keys())\n",
    "    G.add_edges_from(SALBP_dict[\"precedence_relations\"])\n",
    "\n",
    "    edge_colors = []\n",
    "    edge_values = []  # Store unique edge values for legend\n",
    "\n",
    "    for edge in G.edges():\n",
    "        edge_index = SALBP_dict[\"precedence_relations\"].index(list(edge))\n",
    "        no_stations = res_df[(res_df[\"instance:\"] == instance_name) & \n",
    "                             (res_df[\"precedence_relation\"] == edge_index)][\"no_stations\"].values[0]\n",
    "        edge_colors.append(no_stations)\n",
    "        if no_stations not in edge_values:\n",
    "            edge_values.append(no_stations)\n",
    "\n",
    "    # Save edge colors as graph attribute\n",
    "    nx.set_edge_attributes(G, dict(zip(G.edges(), edge_colors)), \"value\")\n",
    "\n",
    "    # Graph layout\n",
    "    pos = nx.nx_pydot.graphviz_layout(G, prog=\"dot\")\n",
    "\n",
    "    # Define discrete colormap\n",
    "    unique_values = sorted(edge_values)\n",
    "    num_colors = len(unique_values)\n",
    "    cmap = plt.cm.get_cmap(\"Set1\", num_colors)  # Use a qualitative colormap\n",
    "    color_map = {val: cmap(i) for i, val in enumerate(unique_values)}  # Assign colors to unique values\n",
    "\n",
    "    # Assign discrete colors to edges\n",
    "    edge_color_list = [color_map[val] for val in edge_colors]\n",
    "\n",
    "    # Draw graph\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    edges = nx.draw_networkx_edges(G, pos, edge_color=edge_color_list, ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    # Create legend\n",
    "    handles = [plt.Line2D([0], [0], color=color_map[val], lw=2, label=f\"No. of Stations: {val}\") for val in unique_values]\n",
    "    #ax.legend(handles=handles, loc=\"best\")\n",
    "\n",
    "\n",
    "    return G\n",
    "\n",
    "i = 1\n",
    "\n",
    "test_salb = parse_alb(f\"/Users/letshopethisworks2/Documents/phd_paper_material/MMABPWW/SALBP_benchmark/small data set_n=20/instance_n=20_{i}.alb\")\n",
    "#test_g = plot_salbp_edge_removal_graph(test_salb, f\"instance_n=20_{i}\", results_df)\n",
    "test_g = draw_graph_with_discrete_legend(test_salb, results_df, f\"instance_n=20_{i}\")\n",
    "#saves graph to a gephi readable file\n",
    "nx.write_gexf(test_g, \"test_salb.gexf\")\n",
    "#plot_salbp_graph(test_salb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_instances['instance:'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a plot of the 27 graphs of interest\n",
    "fig, axs = plt.subplots(5, 11, figsize=(20, 20))\n",
    "axs = axs.ravel()\n",
    "for idx, i in enumerate(interesting_instances['instance:'].values):\n",
    "    test_salb = parse_alb(f\"/Users/letshopethisworks2/Documents/phd_paper_material/MALBP_instance_generation/SALBP_benchmark/small data set_n=20/{i}.alb\")\n",
    "    #test_g = plot_salbp_edge_removal_graph(test_salb, f\"instance_n=20_{i}\", results_df)\n",
    "    test_g = draw_graph_with_discrete_legend(test_salb, results_df, i, ax=axs[idx])\n",
    "    #adds test_g to the axs\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_50 = pd.read_csv(\"SALBP_edge_solutions.csv\")\n",
    "# #changes 20 in instance: to 50\n",
    "# results_50['instance:'] = results_50['instance:'].str.replace(\"20\", \"50\")\n",
    "# #creates a seperate dataframe for the instances with None precedence relations\n",
    "# lb_df_50 = results_50[results_50[\"precedence_relation\"].isna() == True].copy()\n",
    "# #removes the rows with None precedence relations\n",
    "# results_50 = results_50[results_50['precedence_relation'].isna() == False]\n",
    "# #gets the min and max number of stations for each instance\n",
    "# min_and_max_50 = results_50.groupby(\"instance:\")[\"no_stations\"].agg([\"min\", \"max\"])\n",
    "# min_and_max_50.reset_index(inplace = True)\n",
    "# #adds in lb values\n",
    "# lb_df_50['bin_lb'] = lb_df_50['no_stations']\n",
    "# min_and_max_50 = pd.merge(min_and_max_50, lb_df_50[[\"instance:\", \"bin_lb\"]], on = \"instance:\")\n",
    "# #counts the number of times min does not equal max\n",
    "# min_and_max_50[\"min_not_equal_max\"] = min_and_max_50[\"min\"] != min_and_max_50[\"max\"]\n",
    "# min_and_max_50[\"min_not_equal_max\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks at instances where the min is not equal to the max\n",
    "# interesting_instances_50 = min_and_max_50[min_and_max_50[\"min_not_equal_max\"]]\n",
    "# interesting_instances_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df\n",
    "#merges min and max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges results with min and max\n",
    "results_df = pd.merge(results_df, min_and_max, on = \"instance:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_50_1 = pd.read_csv(\"med_50/instances_50_1_through_331.csv\")\n",
    "tasks_50_2 = pd.read_csv(\"med_50/instances_50_331through399.csv\")\n",
    "tasks_50_3 = pd.read_csv(\"med_50/instances_50_400through524.csv\")\n",
    "tasks_50 = pd.concat([ tasks_50_1, tasks_50_2, tasks_50_3])\n",
    "tasks_50.head()\n",
    "\n",
    "#reads the xlsx file\n",
    "task_50_details = pd.read_excel(\"med_50/Details of the medium data set (n=50 and n=50permuted).xlsx\")\n",
    "task_50_details.head()\n",
    "\n",
    "#first row is the columns\n",
    "task_50_details.columns = task_50_details.iloc[0]\n",
    "#removes the first row\n",
    "task_50_details = task_50_details.iloc[1:]\n",
    "task_50_details.head()\n",
    "\n",
    "\n",
    "#removes <> and whitespace from the columns\n",
    "task_50_details.columns = task_50_details.columns.str.replace(\"<\", \"\").str.replace(\">\", \"\").str.strip()\n",
    "task_50_details.columns\n",
    "#left merges on instance: and Filename columns\n",
    "tasks_50 = pd.merge(tasks_50, task_50_details, left_on = \"instance:\", right_on = \"Filename\")\n",
    "tasks_50.head()\n",
    "#drops the Filename column and Unnamed: 0\n",
    "tasks_50.drop(columns = [\"Filename\", \"Unnamed: 0\"], inplace = True)\n",
    "tasks_50.head()\n",
    "\n",
    "\n",
    "#puts the rows with no_stations as none in a seperate dataframe\n",
    "lb_df_50 = tasks_50[tasks_50[\"precedence_relation\"].isna() == True].copy()\n",
    "#removes the rows with None precedence relations\n",
    "tasks_50 = tasks_50[tasks_50['precedence_relation'].isna() == False]\n",
    "#gets the min and max number of stations for each instance\n",
    "min_and_max_50 = tasks_50.groupby(\"instance:\")[\"no_stations\"].agg([\"min\", \"max\"])\n",
    "min_and_max_50.reset_index(inplace = True)\n",
    "# #renames the min and max columns as lowest_cost and original_optimal\n",
    "min_and_max_50.rename(columns = {\"min\": \"lowest_cost\", \"max\": \"original_optimal\"}, inplace = True)\n",
    "# #merges min and max with the tasks_50 dataframe\n",
    "lb_df_50['bin_lb'] = lb_df_50['no_stations']\n",
    "min_and_max_50 = pd.merge(min_and_max_50, lb_df_50[[\"instance:\", \"bin_lb\"]], on = \"instance:\")\n",
    "tasks_50 = pd.merge(tasks_50, min_and_max_50, on = \"instance:\")\n",
    "# #adds in lb values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the instances where min and max have a gap of more than 1\n",
    "bad_eggs = min_and_max_50[min_and_max_50[\"original_optimal\"] - min_and_max_50[\"lowest_cost\"] > 1]\n",
    "bad_eggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_and_max_50[min_and_max_50[\"instance:\"] == \"instance_n=50_31\"\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates bins for order strength 0-0.2, 0.2-0.4, 0.4-0.6, 0.6-0.8, 0.8-1\n",
    "tasks_50['order_strength_bins'] = pd.cut(tasks_50['Order strength'], bins = [0, 0.2, 0.4, 0.6, 0.8, 1], labels = [\"0-0.2\", \"0.2-0.4\", \"0.4-0.6\", \"0.6-0.8\", \"0.8-1\"])\n",
    "tasks_50['min_not_equal_max'] = tasks_50[\"lowest_cost\"] != tasks_50[\"original_optimal\"]\n",
    "tasks_50['bin_lb_less_than_min'] = tasks_50[\"bin_lb\"] < tasks_50[\"lowest_cost\"]\n",
    "tasks_50['bin_lb_less_than_max'] = tasks_50[\"bin_lb\"] < tasks_50[\"original_optimal\"]\n",
    "tasks_50['bin_lb_less_than_max'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intereseting_50 = tasks_50[tasks_50[\"min_not_equal_max\"] == True]\n",
    "boring_50 = tasks_50[tasks_50[\"min_not_equal_max\"] == False]\n",
    "print(\"interesting\", intereseting_50['instance:'].nunique(), \"boring\", boring_50['instance:'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_chain_containing_node(G, node):\n",
    "    if node not in G:\n",
    "        return 0\n",
    "\n",
    "    # Step 1: Get longest path to `node`\n",
    "    longest_to_node = {n: 0 for n in G}  # Dictionary to store longest path to each node\n",
    "    for n in nx.topological_sort(G):  # Process in topological order\n",
    "        for pred in G.predecessors(n):\n",
    "            longest_to_node[n] = max(longest_to_node[n], longest_to_node[pred] + 1)\n",
    "\n",
    "    # Step 2: Get longest path from `node`\n",
    "    longest_from_node = {n: 0 for n in G}  # Dictionary to store longest path from each node\n",
    "    for n in reversed(list(nx.topological_sort(G))):  # Process in reverse topological order\n",
    "        for succ in G.successors(n):\n",
    "            longest_from_node[n] = max(longest_from_node[n], longest_from_node[succ] + 1)\n",
    "\n",
    "    # Step 3: Compute total longest chain containing the node\n",
    "    return longest_to_node[node] + longest_from_node[node] + 1  # +1 to include the node itself\n",
    "\n",
    "\n",
    "def get_longest_chains_edges(G):\n",
    "    # Step 1: Get longest path to `node`\n",
    "    longest_to_node = {n: 0 for n in G}  # Dictionary to store longest path to each node\n",
    "    for n in nx.topological_sort(G):  # Process in topological order\n",
    "        for pred in G.predecessors(n):\n",
    "            longest_to_node[n] = max(longest_to_node[n], longest_to_node[pred] + 1)\n",
    "\n",
    "    # Step 2: Get longest path from `node`\n",
    "    longest_from_node = {n: 0 for n in G}  # Dictionary to store longest path from each node\n",
    "    for n in reversed(list(nx.topological_sort(G))):  # Process in reverse topological order\n",
    "        for succ in G.successors(n):\n",
    "            longest_from_node[n] = max(longest_from_node[n], longest_from_node[succ] + 1)\n",
    "    edge_list = []\n",
    "    # Step 3: Compute total longest chain containing the edge\n",
    "    for edge in G.edges():\n",
    "        edge_list.append((edge, longest_to_node[edge[0]] + longest_from_node[edge[1]] + 1))\n",
    "    return edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from([(0, 1), (0, 2), (1, 3), (2, 3), (3, 4), (5,6), (2,6)] )\n",
    "#adds node attributes \"weight to the nodes\"\n",
    "\n",
    "for node in G.nodes():\n",
    "    G.nodes[node][\"weight\"] = random.randint(1, 10)\n",
    "\n",
    "for n in nx.topological_sort(G):\n",
    "    print(n)\n",
    "\n",
    "#draws the graph\n",
    "nx.draw(G, with_labels = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_all_positional_weight(G):\n",
    "    '''Gets the positional weight of the graph'''\n",
    "    positional_weight = {}\n",
    "    trans_G = nx.transitive_closure(G)\n",
    "    #positional weight is the weight of the node plus the weight of its children\n",
    "    for node in trans_G.nodes():\n",
    "        positional_weight[node] = trans_G.nodes[node][\"weight\"]\n",
    "        for child in trans_G.neighbors(node):\n",
    "            positional_weight[node] += trans_G.nodes[child][\"weight\"]\n",
    "    return positional_weight\n",
    "\n",
    "def get_all_reverse_positional_weight(G):\n",
    "    '''Gets the reverse positional weight of the graph'''\n",
    "    rev_G = G.reverse()\n",
    "    rpw = get_all_positional_weight(rev_G)\n",
    "    return rpw\n",
    "\n",
    "def get_all_children(G):\n",
    "    '''Gets all the children of the nodes in the  graph'''\n",
    "    children_dict = {}\n",
    "    for node in G.nodes():\n",
    "        children_dict[node] = list(G.successors(node))\n",
    "    return children_dict\n",
    "\n",
    "def get_all_parents(G):\n",
    "    '''Gets all the parents of the nodes in the graph'''\n",
    "    parents_dict = {}\n",
    "    for node in G.nodes():\n",
    "        parents_dict[node] = list(G.predecessors(node))\n",
    "    return parents_dict\n",
    "\n",
    "def get_all_succesors(G):\n",
    "    '''Gets all the succesors of the nodes in the graph'''\n",
    "    trans_G = nx.transitive_closure(G)\n",
    "    succesors_dict = {}\n",
    "    for node in trans_G.nodes():\n",
    "        succesors_dict[node] = list(G.predecessors(node))\n",
    "    return succesors_dict\n",
    "\n",
    "def get_edge_neighbor_max_min_avg_std(G):\n",
    "    '''For each edge, gets the maximum and minimum weight of its neighbors'''\n",
    "    edge_neighbor_max_min = {}\n",
    "    for edge in G.edges():\n",
    "        #gets the weights of the predecessors of the first node in the edge\n",
    "        pred_weights = [G.nodes[pred][\"weight\"] for pred in G.predecessors(edge[0])] \n",
    "        print(\"pred weights\", pred_weights)\n",
    "        #gets the weights of the successors of the second node in the edge\n",
    "        succ_weights = [G.nodes[succ][\"weight\"] for succ in G.successors(edge[1])] \n",
    "        print(\"succ weights\", succ_weights)\n",
    "        #adds the max and min of the weights to the edge_neighbor_max_min dictionary\n",
    "        weights = pred_weights + succ_weights\n",
    "        if weights:\n",
    "            edge_neighbor_max_min[edge] = {\"max\": max(weights), \"min\": min(weights), \"avg\": sum(weights)/len(weights), \"std\": np.std(weights)}\n",
    "        else:\n",
    "            edge_neighbor_max_min[edge] = {\"max\": 0, \"min\": 0, \"avg\": 0, \"std\": 0}\n",
    "    return edge_neighbor_max_min\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_edge_neighbor_max_min_avg_std(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_50.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all of the instances where optimal is not equal to original optimal\n",
    "interesting_instances_50 = tasks_50[tasks_50[\"Upper bound on the number of stations\"] != tasks_50[\"original_optimal\"]]\n",
    "interesting_instances_50['instance:'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_instances_50.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_instances_50[['No of stations in optimum', 'original_optimal', 'Optimum found? -- 1 for \"Yes\"','Upper bound on the number of stations', 'bin_lb', \"lowest_cost\"]].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_instances_50[interesting_instances_50['No of stations in optimum'].isin([12,13,34]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
