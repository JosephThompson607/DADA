{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T17:49:55.341329Z",
     "start_time": "2025-06-10T17:49:54.260241Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_alb(alb_file_name):\n",
    "    \"\"\"Reads assembly line balancing instance .alb file, returns dictionary with the information\"\"\"\n",
    "    parse_dict = {}\n",
    "    alb_file = open(alb_file_name).read()\n",
    "    # Get number of tasks\n",
    "    num_tasks = re.search(\"<number of tasks>\\n(\\\\d*)\", alb_file)\n",
    "    parse_dict[\"num_tasks\"] = int(num_tasks.group(1))\n",
    "\n",
    "    # Get cycle time\n",
    "    cycle_time = re.search(\"<cycle time>\\n(\\\\d*)\", alb_file)\n",
    "    parse_dict[\"cycle_time\"] = int(cycle_time.group(1))\n",
    "\n",
    "    # Order Strength\n",
    "    order_strength = re.search(\"<order strength>\\n(\\\\d*,\\\\d*)\", alb_file)\n",
    "    \n",
    "    if order_strength:\n",
    "        parse_dict[\"original_order_strength\"] = float(order_strength.group(1).replace(\",\", \".\"))\n",
    "    else:\n",
    "        order_strength = re.search(\"<order strength>\\n(\\\\d*.\\\\d*)\", alb_file)\n",
    "        parse_dict[\"original_order_strength\"] = float(order_strength.group(1))\n",
    "\n",
    "    # Task_times\n",
    "    task_times = re.search(\"<task times>(.|\\n)+?<\", alb_file)\n",
    "\n",
    "    # Get lines in this regex ignoring the first and last 2\n",
    "    task_times = task_times.group(0).split(\"\\n\")[1:-2]\n",
    "    task_times = {task.split()[0]: int(task.split()[1]) for task in task_times}\n",
    "    parse_dict[\"task_times\"] = task_times\n",
    "\n",
    "    # Precedence relations\n",
    "    precedence_relations = re.search(\"<precedence relations>(.|\\n)+?<\", alb_file)\n",
    "    precedence_relations = precedence_relations.group(0).split(\"\\n\")[1:-2]\n",
    "    precedence_relations = [task.split(\",\") for task in precedence_relations]\n",
    "    parse_dict[\"precedence_relations\"] = precedence_relations\n",
    "    return parse_dict\n",
    "\n",
    "def write_to_alb(salbp_dict, alb_file_name):\n",
    "    \"\"\"Writes the SALBP dictionary to an .alb file\"\"\"\n",
    "    #Format of alb:\n",
    "    # <number of tasks>\n",
    "    # no_tasks\n",
    "    # <cycle time>\n",
    "    # cycle_time\n",
    "    #<task times>\n",
    "    #task_id task_time\n",
    "    #<precedence relations>\n",
    "    #task_id,task_id\n",
    "\n",
    "\n",
    "    # Write number of tasks\n",
    "    alb = \"<number of tasks>\\n\"\n",
    "    alb += str(salbp_dict[\"num_tasks\"]) + \"\\n\"\n",
    "    # Write cycle time\n",
    "    alb += \"<cycle time>\\n\"\n",
    "    alb += str(salbp_dict[\"cycle_time\"]) + \"\\n\"\n",
    "    # Write task times\n",
    "    alb += \"<task times>\\n\"\n",
    "    for task_id, task_time in salbp_dict[\"task_times\"].items():\n",
    "        alb += task_id + \" \" + str(task_time) + \"\\n\"\n",
    "    # Write precedence relations\n",
    "    alb += \"<precedence relations>\\n\"\n",
    "    for relation in salbp_dict[\"precedence_relations\"]:\n",
    "        alb += relation[0] + \",\" + relation[1] + \"\\n\"\n",
    "    #ends the file\n",
    "    alb += \"<end>\"\n",
    "    with open(alb_file_name, \"w\") as alb_file:\n",
    "        alb_file.write(alb)\n",
    "    \n",
    "\n",
    "\n",
    "SALBP_dict = parse_alb(\"/Users/letshopethisworks2/Documents/phd_paper_material/MALBP_instance_generation/SALBP_benchmark/small data set_n=20/instance_n=20_1.alb\")\n",
    "write_to_alb(SALBP_dict, \"test.alb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_alb = {'num_tasks': 12,\n",
    " 'cycle_time': 10,\n",
    " 'original_order_strength': 0.268,\n",
    " 'task_times': {'1': 3,\n",
    "  '2': 8,\n",
    "  '3': 3,\n",
    "  '4': 2,\n",
    "  '5': 4,\n",
    "  '6': 2,\n",
    "  '7': 5,\n",
    "  '8': 1,\n",
    "  '9': 9,\n",
    "  '10': 4,\n",
    "  '11': 6,\n",
    "  '12': 2,\n",
    "    },\n",
    " 'precedence_relations': [\n",
    "     ['1', '5'],\n",
    "  ['2', '5'],\n",
    "  ['3', '5'],\n",
    "  ['3', '4'],\n",
    "  ['4', '9'],\n",
    "  ['5', '6'],\n",
    "  ['5', '7'],\n",
    "  ['5', '8'],\n",
    "  ['6', '11'],\n",
    "  ['7', '11'],\n",
    "  ['8', '11'],\n",
    "  ['9', '10'],\n",
    "  ['10', '12'],\n",
    "  ['11', '12']\n",
    "  ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SALBP_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls BBR-for-SALBP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ../BBR-for-SALBP1/SALB/SALB/salb  -p 2 \"test.alb\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def random_task_time_change(SALBP_dict, multiplier = 1.5):\n",
    "    \"\"\"Increases a random task time by 1\"\"\"\n",
    "    import random\n",
    "    task_id = random.choice(list(SALBP_dict[\"task_times\"].keys()))\n",
    "    SALBP_dict[\"task_times\"][task_id] *= multiplier\n",
    "    return SALBP_dict\n",
    "\n",
    "def task_time_change(SALBP_dict, task_id, multiplier = 1.5, debug = False):\n",
    "    \"\"\"Increases a random task time by 1\"\"\"\n",
    "    if debug:\n",
    "        print(\"Changing task\", task_id, \"time by\", multiplier)\n",
    "    SALBP_dict[\"task_times\"][task_id] *= multiplier\n",
    "    return SALBP_dict\n",
    "\n",
    "def precedence_removal(SALBP_dict, edge_index):\n",
    "    \"\"\"Removes a precedence relation\"\"\"\n",
    "    SALBP_dict[\"precedence_relations\"].pop(edge_index)\n",
    "    return SALBP_dict\n",
    "    \n",
    "\n",
    "def parse_bb_salb1_out(text):\n",
    "    '''gets the number of stations, optimal flag and cpu time from the output of the salb1 program'''\n",
    "    output = text.stdout.decode(\"utf-8\")\n",
    "    # Regular expression to capture the required values\n",
    "    match = re.search(r\"verified_optimality\\s*=\\s*(\\d+);\\s*value\\s*=\\s*(\\d+);\\s*cpu\\s*=\\s*([\\d.]+)\", output)\n",
    "\n",
    "    if match:\n",
    "        verified_optimality = int(match.group(1))\n",
    "        value = int(match.group(2))\n",
    "        cpu = float(match.group(3))\n",
    "\n",
    "    else:\n",
    "        print(\"Pattern not found.\")\n",
    "    return value, verified_optimality, cpu\n",
    "\n",
    "def generate_results(fp = \"/Users/letshopethisworks2/Documents/phd_paper_material/MALBP_instance_generation/SALBP_benchmark/small data set_n=20/\" ,  instance_name = \"instance_n=20_\", ext = \".alb\", start=1, stop = 300):\n",
    "    results = []\n",
    "    for i in range(start,stop):\n",
    "        SALBP_dict_orig = parse_alb(f\"{fp}{instance_name}{i}{ext}\")\n",
    "        bin_dict = deepcopy(SALBP_dict_orig)\n",
    "        print(\"Running instance: \", i)\n",
    "        for j in range(len(SALBP_dict_orig[\"precedence_relations\"])):\n",
    "            SALBP_dict = deepcopy(SALBP_dict_orig)\n",
    "            SALBP_dict =precedence_removal(SALBP_dict, j)\n",
    "            write_to_alb(SALBP_dict, \"test.alb\")\n",
    "            output = subprocess.run([ex_fp, \"test.alb\"], stdout=subprocess.PIPE)\n",
    "            no_stations, optimal, cpu = parse_bb_salb1_out(output)\n",
    "            result = {\"instance:\": f\"instance_n=20_{i}\", \"precedence_relation\": j, \"no_stations\": no_stations, \"optimal\": optimal, \"cpu\": cpu}\n",
    "            save_backup(backup_name, result)\n",
    "            results.append(result)\n",
    "\n",
    "        #calculates bin packing lower bound\n",
    "        bin_dict['precedence_relations'] = []\n",
    "        write_to_alb(bin_dict, \"test.alb\")\n",
    "        output = subprocess.run([ex_fp, \"test.alb\"], stdout=subprocess.PIPE)\n",
    "        no_stations, optimal, cpu = parse_bb_salb1_out(output)\n",
    "        result = {\"instance:\": f\"instance_n=20_{i}\", \"precedence_relation\": \"None\", \"no_stations\": no_stations, \"optimal\": optimal, \"cpu\": cpu}\n",
    "        save_backup(backup_name, result)\n",
    "            \n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "#reads the results csv\n",
    "#results_df = pd.read_csv(\"task_20_bin_lb.csv\")\n",
    "#results_df = pd.DataFrame(results)\n",
    "#saves the results df to a csv file\n",
    "#results_df.to_csv(\"tasks20_test.csv\")\n",
    "# results = generate_results(start=400, stop = 525)\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(\"tasks20_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = pd.read_csv(\"task_20_bin_lb.csv\")\n",
    "res_2 = pd.read_csv(\"tasks20_2.csv\")\n",
    "res_3 = pd.read_csv(\"tasks20_3.csv\")\n",
    "\n",
    "results_df = pd.concat([res_1, res_2, res_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_df = results_df[results_df[\"precedence_relation\"].isna() == True].copy()\n",
    "#removes the rows with None precedence relations\n",
    "results_df = results_df[results_df['precedence_relation'].isna() == False]\n",
    "\n",
    "lb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the min and max number of stations for each instance\n",
    "min_and_max = results_df.groupby(\"instance:\")[\"no_stations\"].agg([\"min\", \"max\"])\n",
    "min_and_max.reset_index(inplace = True)\n",
    "#adds in lb values\n",
    "lb_df['bin_lb'] = lb_df['no_stations']\n",
    "min_and_max = pd.merge(min_and_max, lb_df[[\"instance:\", \"bin_lb\"]], on = \"instance:\")\n",
    "min_and_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts the number of times min does not equal max\n",
    "min_and_max[\"min_not_equal_max\"] = min_and_max[\"min\"] != min_and_max[\"max\"]\n",
    "min_and_max[\"min_not_equal_max\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts the number of time the bin_lb is less than the min\n",
    "min_and_max[\"bin_lb_less_than_min\"] = min_and_max[\"bin_lb\"] < min_and_max[\"min\"]\n",
    "min_and_max[\"bin_lb_less_than_min\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts the number of time the bin_lb is less than the max\n",
    "min_and_max[\"bin_lb_less_than_max\"] = min_and_max[\"bin_lb\"] < min_and_max[\"max\"]\n",
    "print(\"bin lb dif\", min_and_max[\"bin_lb_less_than_max\"].sum())\n",
    "#filters for the instances where the bin_lb is les than the max\n",
    "min_and_max[min_and_max[\"bin_lb_less_than_max\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the instances where min does not equal max\n",
    "interesting_instances = min_and_max[min_and_max[\"min_not_equal_max\"]]\n",
    "interesting_instances['instance:'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_20_101 = results_df[results_df[\"instance:\"] == \"instance_n=20_101\"]\n",
    "inst_20_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_salbp_graph(SALBP_dict):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(SALBP_dict[\"task_times\"].keys())\n",
    "    G.add_edges_from(SALBP_dict[\"precedence_relations\"])\n",
    "    #prints the edges\n",
    "    print(\"from dict\", SALBP_dict[\"precedence_relations\"])\n",
    "    #prints the edges from the graph\n",
    "    print(\"from graph\", G.edges())\n",
    "    nx.draw(G, with_labels = True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_salbp_edge_removal_graph(SALBP_dict, instance_name, res_df):\n",
    "    '''Colors the edges by the number of stations in res_df'''\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(SALBP_dict[\"task_times\"].keys())\n",
    "    G.add_edges_from(SALBP_dict[\"precedence_relations\"])\n",
    "    edge_colors = []\n",
    "    for edge in G.edges():\n",
    "        edge_index = SALBP_dict[\"precedence_relations\"].index(list(edge))\n",
    "        no_stations = res_df[(res_df[\"instance:\"] == instance_name) & (res_df[\"precedence_relation\"] == edge_index)][\"no_stations\"].values[0]\n",
    "        edge_colors.append(no_stations)\n",
    "    #saves edge colors as graph attribute\n",
    "    nx.set_edge_attributes(G, dict(zip(G.edges(), edge_colors)), \"value\")\n",
    "    pos = nx.nx_pydot.graphviz_layout(G, prog = \"dot\")\n",
    "   # Define colormap\n",
    "    unique_values = list(set(edge_colors))\n",
    "    print(unique_values)\n",
    "    color_map = cm.get_cmap('viridis', len(unique_values))\n",
    "    print(\"color map\", color_map)\n",
    "    cmap = mcolors.ListedColormap([color_map(val) for val in unique_values])\n",
    "\n",
    "    # Draw graph\n",
    "    #creates ax\n",
    "    fig, ax = plt.subplots()\n",
    "    edges = nx.draw_networkx_edges(G, pos, edge_color=edge_colors, edge_cmap=cmap, edge_vmin=min(edge_colors), edge_vmax=max(edge_colors), ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax)\n",
    "\n",
    "    # Add colorbar\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color = color_map(val), label=val, markersize=10) for val in unique_values]\n",
    "    plt.legend(handles=handles, loc=\"best\")\n",
    "\n",
    "    plt.show()\n",
    "    return G\n",
    "\n",
    "\n",
    "def draw_graph_with_discrete_legend(SALBP_dict, res_df, instance_name,  ax=None):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(SALBP_dict[\"task_times\"].keys())\n",
    "    G.add_edges_from(SALBP_dict[\"precedence_relations\"])\n",
    "\n",
    "    edge_colors = []\n",
    "    edge_values = []  # Store unique edge values for legend\n",
    "\n",
    "    for edge in G.edges():\n",
    "        edge_index = SALBP_dict[\"precedence_relations\"].index(list(edge))\n",
    "        no_stations = res_df[(res_df[\"instance:\"] == instance_name) & \n",
    "                             (res_df[\"precedence_relation\"] == edge_index)][\"no_stations\"].values[0]\n",
    "        edge_colors.append(no_stations)\n",
    "        if no_stations not in edge_values:\n",
    "            edge_values.append(no_stations)\n",
    "\n",
    "    # Save edge colors as graph attribute\n",
    "    nx.set_edge_attributes(G, dict(zip(G.edges(), edge_colors)), \"value\")\n",
    "\n",
    "    # Graph layout\n",
    "    pos = nx.nx_pydot.graphviz_layout(G, prog=\"dot\")\n",
    "\n",
    "    # Define discrete colormap\n",
    "    unique_values = sorted(edge_values)\n",
    "    num_colors = len(unique_values)\n",
    "    cmap = plt.cm.get_cmap(\"Set1\", num_colors)  # Use a qualitative colormap\n",
    "    color_map = {val: cmap(i) for i, val in enumerate(unique_values)}  # Assign colors to unique values\n",
    "\n",
    "    # Assign discrete colors to edges\n",
    "    edge_color_list = [color_map[val] for val in edge_colors]\n",
    "\n",
    "    # Draw graph\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    edges = nx.draw_networkx_edges(G, pos, edge_color=edge_color_list, ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    # Create legend\n",
    "    handles = [plt.Line2D([0], [0], color=color_map[val], lw=2, label=f\"No. of Stations: {val}\") for val in unique_values]\n",
    "    #ax.legend(handles=handles, loc=\"best\")\n",
    "\n",
    "\n",
    "    return G\n",
    "\n",
    "i = 1\n",
    "\n",
    "test_salb = parse_alb(f\"/Users/letshopethisworks2/Documents/phd_paper_material/MMABPWW/SALBP_benchmark/small data set_n=20/instance_n=20_{i}.alb\")\n",
    "#test_g = plot_salbp_edge_removal_graph(test_salb, f\"instance_n=20_{i}\", results_df)\n",
    "test_g = draw_graph_with_discrete_legend(test_salb, results_df, f\"instance_n=20_{i}\")\n",
    "#saves graph to a gephi readable file\n",
    "nx.write_gexf(test_g, \"test_salb.gexf\")\n",
    "#plot_salbp_graph(test_salb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_instances['instance:'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a plot of the 27 graphs of interest\n",
    "fig, axs = plt.subplots(5, 11, figsize=(20, 20))\n",
    "axs = axs.ravel()\n",
    "for idx, i in enumerate(interesting_instances['instance:'].values):\n",
    "    test_salb = parse_alb(f\"/Users/letshopethisworks2/Documents/phd_paper_material/MALBP_instance_generation/SALBP_benchmark/small data set_n=20/{i}.alb\")\n",
    "    #test_g = plot_salbp_edge_removal_graph(test_salb, f\"instance_n=20_{i}\", results_df)\n",
    "    test_g = draw_graph_with_discrete_legend(test_salb, results_df, i, ax=axs[idx])\n",
    "    #adds test_g to the axs\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_50 = pd.read_csv(\"SALBP_edge_solutions.csv\")\n",
    "# #changes 20 in instance: to 50\n",
    "# results_50['instance:'] = results_50['instance:'].str.replace(\"20\", \"50\")\n",
    "# #creates a seperate dataframe for the instances with None precedence relations\n",
    "# lb_df_50 = results_50[results_50[\"precedence_relation\"].isna() == True].copy()\n",
    "# #removes the rows with None precedence relations\n",
    "# results_50 = results_50[results_50['precedence_relation'].isna() == False]\n",
    "# #gets the min and max number of stations for each instance\n",
    "# min_and_max_50 = results_50.groupby(\"instance:\")[\"no_stations\"].agg([\"min\", \"max\"])\n",
    "# min_and_max_50.reset_index(inplace = True)\n",
    "# #adds in lb values\n",
    "# lb_df_50['bin_lb'] = lb_df_50['no_stations']\n",
    "# min_and_max_50 = pd.merge(min_and_max_50, lb_df_50[[\"instance:\", \"bin_lb\"]], on = \"instance:\")\n",
    "# #counts the number of times min does not equal max\n",
    "# min_and_max_50[\"min_not_equal_max\"] = min_and_max_50[\"min\"] != min_and_max_50[\"max\"]\n",
    "# min_and_max_50[\"min_not_equal_max\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks at instances where the min is not equal to the max\n",
    "# interesting_instances_50 = min_and_max_50[min_and_max_50[\"min_not_equal_max\"]]\n",
    "# interesting_instances_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df\n",
    "#merges min and max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges results with min and max\n",
    "results_df = pd.merge(results_df, min_and_max, on = \"instance:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_50_1 = pd.read_csv(\"med_50/instances_50_1_through_331.csv\")\n",
    "tasks_50_2 = pd.read_csv(\"med_50/instances_50_331through399.csv\")\n",
    "tasks_50_3 = pd.read_csv(\"med_50/instances_50_400through524.csv\")\n",
    "tasks_50 = pd.concat([ tasks_50_1, tasks_50_2, tasks_50_3])\n",
    "tasks_50.head()\n",
    "\n",
    "#reads the xlsx file\n",
    "task_50_details = pd.read_excel(\"med_50/Details of the medium data set (n=50 and n=50permuted).xlsx\")\n",
    "task_50_details.head()\n",
    "\n",
    "#first row is the columns\n",
    "task_50_details.columns = task_50_details.iloc[0]\n",
    "#removes the first row\n",
    "task_50_details = task_50_details.iloc[1:]\n",
    "task_50_details.head()\n",
    "\n",
    "\n",
    "#removes <> and whitespace from the columns\n",
    "task_50_details.columns = task_50_details.columns.str.replace(\"<\", \"\").str.replace(\">\", \"\").str.strip()\n",
    "task_50_details.columns\n",
    "#left merges on instance: and Filename columns\n",
    "tasks_50 = pd.merge(tasks_50, task_50_details, left_on = \"instance:\", right_on = \"Filename\")\n",
    "tasks_50.head()\n",
    "#drops the Filename column and Unnamed: 0\n",
    "tasks_50.drop(columns = [\"Filename\", \"Unnamed: 0\"], inplace = True)\n",
    "tasks_50.head()\n",
    "\n",
    "\n",
    "#puts the rows with no_stations as none in a seperate dataframe\n",
    "lb_df_50 = tasks_50[tasks_50[\"precedence_relation\"].isna() == True].copy()\n",
    "#removes the rows with None precedence relations\n",
    "tasks_50 = tasks_50[tasks_50['precedence_relation'].isna() == False]\n",
    "#gets the min and max number of stations for each instance\n",
    "min_and_max_50 = tasks_50.groupby(\"instance:\")[\"no_stations\"].agg([\"min\", \"max\"])\n",
    "min_and_max_50.reset_index(inplace = True)\n",
    "# #renames the min and max columns as lowest_cost and original_optimal\n",
    "min_and_max_50.rename(columns = {\"min\": \"lowest_cost\", \"max\": \"original_optimal\"}, inplace = True)\n",
    "# #merges min and max with the tasks_50 dataframe\n",
    "lb_df_50['bin_lb'] = lb_df_50['no_stations']\n",
    "min_and_max_50 = pd.merge(min_and_max_50, lb_df_50[[\"instance:\", \"bin_lb\"]], on = \"instance:\")\n",
    "tasks_50 = pd.merge(tasks_50, min_and_max_50, on = \"instance:\")\n",
    "# #adds in lb values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the instances where min and max have a gap of more than 1\n",
    "bad_eggs = min_and_max_50[min_and_max_50[\"original_optimal\"] - min_and_max_50[\"lowest_cost\"] > 1]\n",
    "bad_eggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_and_max_50[min_and_max_50[\"instance:\"] == \"instance_n=50_31\"\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates bins for order strength 0-0.2, 0.2-0.4, 0.4-0.6, 0.6-0.8, 0.8-1\n",
    "tasks_50['order_strength_bins'] = pd.cut(tasks_50['Order strength'], bins = [0, 0.2, 0.4, 0.6, 0.8, 1], labels = [\"0-0.2\", \"0.2-0.4\", \"0.4-0.6\", \"0.6-0.8\", \"0.8-1\"])\n",
    "tasks_50['min_not_equal_max'] = tasks_50[\"lowest_cost\"] != tasks_50[\"original_optimal\"]\n",
    "tasks_50['bin_lb_less_than_min'] = tasks_50[\"bin_lb\"] < tasks_50[\"lowest_cost\"]\n",
    "tasks_50['bin_lb_less_than_max'] = tasks_50[\"bin_lb\"] < tasks_50[\"original_optimal\"]\n",
    "tasks_50['bin_lb_less_than_max'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intereseting_50 = tasks_50[tasks_50[\"min_not_equal_max\"] == True]\n",
    "boring_50 = tasks_50[tasks_50[\"min_not_equal_max\"] == False]\n",
    "print(\"interesting\", intereseting_50['instance:'].nunique(), \"boring\", boring_50['instance:'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_chain_containing_node(G, node):\n",
    "    if node not in G:\n",
    "        return 0\n",
    "\n",
    "    # Step 1: Get longest path to `node`\n",
    "    longest_to_node = {n: 0 for n in G}  # Dictionary to store longest path to each node\n",
    "    for n in nx.topological_sort(G):  # Process in topological order\n",
    "        for pred in G.predecessors(n):\n",
    "            longest_to_node[n] = max(longest_to_node[n], longest_to_node[pred] + 1)\n",
    "\n",
    "    # Step 2: Get longest path from `node`\n",
    "    longest_from_node = {n: 0 for n in G}  # Dictionary to store longest path from each node\n",
    "    for n in reversed(list(nx.topological_sort(G))):  # Process in reverse topological order\n",
    "        for succ in G.successors(n):\n",
    "            longest_from_node[n] = max(longest_from_node[n], longest_from_node[succ] + 1)\n",
    "\n",
    "    # Step 3: Compute total longest chain containing the node\n",
    "    return longest_to_node[node] + longest_from_node[node] + 1  # +1 to include the node itself\n",
    "\n",
    "\n",
    "def get_longest_chains_edges(G):\n",
    "    # Step 1: Get longest path to `node`\n",
    "    longest_to_node = {n: 0 for n in G}  # Dictionary to store longest path to each node\n",
    "    for n in nx.topological_sort(G):  # Process in topological order\n",
    "        for pred in G.predecessors(n):\n",
    "            longest_to_node[n] = max(longest_to_node[n], longest_to_node[pred] + 1)\n",
    "\n",
    "    # Step 2: Get longest path from `node`\n",
    "    longest_from_node = {n: 0 for n in G}  # Dictionary to store longest path from each node\n",
    "    for n in reversed(list(nx.topological_sort(G))):  # Process in reverse topological order\n",
    "        for succ in G.successors(n):\n",
    "            longest_from_node[n] = max(longest_from_node[n], longest_from_node[succ] + 1)\n",
    "    edge_list = []\n",
    "    # Step 3: Compute total longest chain containing the edge\n",
    "    for edge in G.edges():\n",
    "        edge_list.append((edge, longest_to_node[edge[0]] + longest_from_node[edge[1]] + 1))\n",
    "    return edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from([(0, 1), (0, 2), (1, 3), (2, 3), (3, 4), (5,6), (2,6)] )\n",
    "#adds node attributes \"weight to the nodes\"\n",
    "\n",
    "for node in G.nodes():\n",
    "    G.nodes[node][\"weight\"] = random.randint(1, 10)\n",
    "\n",
    "for n in nx.topological_sort(G):\n",
    "    print(n)\n",
    "\n",
    "#draws the graph\n",
    "nx.draw(G, with_labels = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.nodes(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_all_positional_weight(G):\n",
    "    '''Gets the positional weight of the graph'''\n",
    "    positional_weight = {}\n",
    "    trans_G = nx.transitive_closure(G)\n",
    "    #positional weight is the weight of the node plus the weight of its children\n",
    "    for node in trans_G.nodes():\n",
    "        positional_weight[node] = trans_G.nodes[node][\"weight\"]\n",
    "        for child in trans_G.neighbors(node):\n",
    "            positional_weight[node] += trans_G.nodes[child][\"weight\"]\n",
    "    return positional_weight\n",
    "\n",
    "def get_all_reverse_positional_weight(G):\n",
    "    '''Gets the reverse positional weight of the graph'''\n",
    "    rev_G = G.reverse()\n",
    "    rpw = get_all_positional_weight(rev_G)\n",
    "    return rpw\n",
    "\n",
    "def get_all_children(G):\n",
    "    '''Gets all the children of the nodes in the  graph'''\n",
    "    children_dict = {}\n",
    "    for node in G.nodes():\n",
    "        children_dict[node] = list(G.successors(node))\n",
    "    return children_dict\n",
    "\n",
    "def get_all_parents(G):\n",
    "    '''Gets all the parents of the nodes in the graph'''\n",
    "    parents_dict = {}\n",
    "    for node in G.nodes():\n",
    "        parents_dict[node] = list(G.predecessors(node))\n",
    "    return parents_dict\n",
    "\n",
    "def get_all_succesors(G):\n",
    "    '''Gets all the succesors of the nodes in the graph'''\n",
    "    trans_G = nx.transitive_closure(G)\n",
    "    succesors_dict = {}\n",
    "    for node in trans_G.nodes():\n",
    "        succesors_dict[node] = list(G.predecessors(node))\n",
    "    return succesors_dict\n",
    "\n",
    "def get_edge_neighbor_max_min_avg_std(G):\n",
    "    '''For each edge, gets the maximum and minimum weight of its neighbors'''\n",
    "    edge_neighbor_max_min = {}\n",
    "    for edge in G.edges():\n",
    "        #gets the weights of the predecessors of the first node in the edge\n",
    "        pred_weights = [G.nodes[pred][\"weight\"] for pred in G.predecessors(edge[0])] \n",
    "        print(\"pred weights\", pred_weights)\n",
    "        #gets the weights of the successors of the second node in the edge\n",
    "        succ_weights = [G.nodes[succ][\"weight\"] for succ in G.successors(edge[1])] \n",
    "        print(\"succ weights\", succ_weights)\n",
    "        #adds the max and min of the weights to the edge_neighbor_max_min dictionary\n",
    "        weights = pred_weights + succ_weights\n",
    "        if weights:\n",
    "            edge_neighbor_max_min[edge] = {\"max\": max(weights), \"min\": min(weights), \"avg\": sum(weights)/len(weights), \"std\": np.std(weights)}\n",
    "        else:\n",
    "            edge_neighbor_max_min[edge] = {\"max\": 0, \"min\": 0, \"avg\": 0, \"std\": 0}\n",
    "    return edge_neighbor_max_min\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_edge_neighbor_max_min_avg_std(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_50.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all of the instances where optimal is not equal to original optimal\n",
    "interesting_instances_50 = tasks_50[tasks_50[\"Upper bound on the number of stations\"] != tasks_50[\"original_optimal\"]]\n",
    "interesting_instances_50['instance:'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_instances_50.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_instances_50[['No of stations in optimum', 'original_optimal', 'Optimum found? -- 1 for \"Yes\"','Upper bound on the number of stations', 'bin_lb', \"lowest_cost\"]].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_instances_50[interesting_instances_50['No of stations in optimum'].isin([12,13,34]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
